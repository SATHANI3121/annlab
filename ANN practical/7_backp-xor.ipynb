{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc26774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.2757478220867212\n",
      "Epoch 100: Loss = 0.25010508840410856\n",
      "Epoch 200: Loss = 0.25008211106582867\n",
      "Epoch 300: Loss = 0.2500631699096177\n",
      "Epoch 400: Loss = 0.25004579083444667\n",
      "Epoch 500: Loss = 0.2500295408963493\n",
      "Epoch 600: Loss = 0.25001403259402094\n",
      "Epoch 700: Loss = 0.249998906754766\n",
      "Epoch 800: Loss = 0.2499838175203082\n",
      "Epoch 900: Loss = 0.24996841825330562\n",
      "Epoch 1000: Loss = 0.24995234742502098\n",
      "Epoch 1100: Loss = 0.2499352135612775\n",
      "Epoch 1200: Loss = 0.24991657825623936\n",
      "Epoch 1300: Loss = 0.249895936107716\n",
      "Epoch 1400: Loss = 0.24987269017042418\n",
      "Epoch 1500: Loss = 0.249846121142314\n",
      "Epoch 1600: Loss = 0.24981534796073482\n",
      "Epoch 1700: Loss = 0.2497792767462103\n",
      "Epoch 1800: Loss = 0.24973653403860174\n",
      "Epoch 1900: Loss = 0.24968537896595416\n",
      "Epoch 2000: Loss = 0.24962358732314793\n",
      "Epoch 2100: Loss = 0.24954829851107563\n",
      "Epoch 2200: Loss = 0.2494558139995795\n",
      "Epoch 2300: Loss = 0.24934133375266518\n",
      "Epoch 2400: Loss = 0.24919861562274065\n",
      "Epoch 2500: Loss = 0.24901954346844263\n",
      "Epoch 2600: Loss = 0.24879359495225992\n",
      "Epoch 2700: Loss = 0.2485072127304812\n",
      "Epoch 2800: Loss = 0.2481431060387816\n",
      "Epoch 2900: Loss = 0.24767954366248102\n",
      "Epoch 3000: Loss = 0.2470897365039681\n",
      "Epoch 3100: Loss = 0.24634142885853516\n",
      "Epoch 3200: Loss = 0.245396794428548\n",
      "Epoch 3300: Loss = 0.24421265514158588\n",
      "Epoch 3400: Loss = 0.2427409600584252\n",
      "Epoch 3500: Loss = 0.24092952241038373\n",
      "Epoch 3600: Loss = 0.23872338927236877\n",
      "Epoch 3700: Loss = 0.2360679224042631\n",
      "Epoch 3800: Loss = 0.23291531063562854\n",
      "Epoch 3900: Loss = 0.22923592703964307\n",
      "Epoch 4000: Loss = 0.22503364950591814\n",
      "Epoch 4100: Loss = 0.22035998682357727\n",
      "Epoch 4200: Loss = 0.21531831669264717\n",
      "Epoch 4300: Loss = 0.21005151515783108\n",
      "Epoch 4400: Loss = 0.204715047387578\n",
      "Epoch 4500: Loss = 0.19944667393137866\n",
      "Epoch 4600: Loss = 0.19434506099091445\n",
      "Epoch 4700: Loss = 0.18946271369189033\n",
      "Epoch 4800: Loss = 0.18481072469077395\n",
      "Epoch 4900: Loss = 0.18036897043093245\n",
      "Epoch 5000: Loss = 0.17609545438220592\n",
      "Epoch 5100: Loss = 0.17192999120425115\n",
      "Epoch 5200: Loss = 0.16778839482085667\n",
      "Epoch 5300: Loss = 0.1635428177708742\n",
      "Epoch 5400: Loss = 0.15898166609906964\n",
      "Epoch 5500: Loss = 0.15374222435781137\n",
      "Epoch 5600: Loss = 0.14723480307718562\n",
      "Epoch 5700: Loss = 0.1387033736989653\n",
      "Epoch 5800: Loss = 0.12774052397281926\n",
      "Epoch 5900: Loss = 0.11495699210759786\n",
      "Epoch 6000: Loss = 0.10154941609603065\n",
      "Epoch 6100: Loss = 0.088446838852551\n",
      "Epoch 6200: Loss = 0.07626961024547675\n",
      "Epoch 6300: Loss = 0.06543311843649058\n",
      "Epoch 6400: Loss = 0.056120641443392336\n",
      "Epoch 6500: Loss = 0.04830600934920008\n",
      "Epoch 6600: Loss = 0.04183477319628158\n",
      "Epoch 6700: Loss = 0.036503081676447724\n",
      "Epoch 6800: Loss = 0.03210753990237227\n",
      "Epoch 6900: Loss = 0.028468403729727577\n",
      "Epoch 7000: Loss = 0.02543635385827888\n",
      "Epoch 7100: Loss = 0.02289129621916025\n",
      "Epoch 7200: Loss = 0.020738172900766218\n",
      "Epoch 7300: Loss = 0.018902219464308763\n",
      "Epoch 7400: Loss = 0.01732466595442045\n",
      "Epoch 7500: Loss = 0.015959179712730552\n",
      "Epoch 7600: Loss = 0.014769049845575713\n",
      "Epoch 7700: Loss = 0.013725008091681192\n",
      "Epoch 7800: Loss = 0.012803559959566996\n",
      "Epoch 7900: Loss = 0.011985711357397628\n",
      "Epoch 8000: Loss = 0.01125599654384455\n",
      "Epoch 8100: Loss = 0.010601733857006756\n",
      "Epoch 8200: Loss = 0.01001245322519589\n",
      "Epoch 8300: Loss = 0.009479453364865203\n",
      "Epoch 8400: Loss = 0.008995457196852923\n",
      "Epoch 8500: Loss = 0.008554341983554543\n",
      "Epoch 8600: Loss = 0.00815092661432466\n",
      "Epoch 8700: Loss = 0.007780802853532662\n",
      "Epoch 8800: Loss = 0.007440200613537111\n",
      "Epoch 8900: Loss = 0.007125879724073606\n",
      "Epoch 9000: Loss = 0.006835042462882244\n",
      "Epoch 9100: Loss = 0.006565262453156132\n",
      "Epoch 9200: Loss = 0.006314426540798452\n",
      "Epoch 9300: Loss = 0.006080687025477892\n",
      "Epoch 9400: Loss = 0.005862422197544891\n",
      "Epoch 9500: Loss = 0.005658203574478453\n",
      "Epoch 9600: Loss = 0.00546676856980382\n",
      "Epoch 9700: Loss = 0.005286997589545921\n",
      "Epoch 9800: Loss = 0.005117894754926532\n",
      "Epoch 9900: Loss = 0.004958571609085388\n",
      "Predictions:\n",
      "[[0.0706824 ]\n",
      " [0.93356795]\n",
      " [0.93308484]\n",
      " [0.07311686]]\n"
     ]
    }
   ],
   "source": [
    "'''Write a python program to show Back Propagation Network for XOR function with Binary Input\n",
    "and Output'''\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the neural network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Initialize weights and biases randomly\n",
    "        self.weights_input_hidden = np.random.rand(2, 2)\n",
    "        self.bias_input_hidden = np.random.rand(1, 2)\n",
    "        self.weights_hidden_output = np.random.rand(2, 1)\n",
    "        self.bias_hidden_output = np.random.rand(1, 1)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        # Forward propagation\n",
    "        self.hidden_output = sigmoid(np.dot(X, self.weights_input_hidden) + self.bias_input_hidden)\n",
    "        self.output = sigmoid(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_hidden_output)\n",
    "        return self.output\n",
    "\n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        # Backpropagation\n",
    "        output_error = y - self.output\n",
    "        output_delta = output_error * sigmoid_derivative(self.output)\n",
    "\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * learning_rate\n",
    "        self.bias_hidden_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "        self.weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
    "        self.bias_input_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            output = self.feedforward(X)\n",
    "            # Backpropagation\n",
    "            self.backpropagation(X, y, learning_rate)\n",
    "            # Print the loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean(np.square(y - output))\n",
    "                print(f\"Epoch {epoch}: Loss = {loss}\")\n",
    "\n",
    "\n",
    "\n",
    "# Define the input data and labels for XOR function\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Create a neural network instance\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Train the neural network\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "nn.train(X, y, epochs, learning_rate)\n",
    "\n",
    "# Test the trained model\n",
    "test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "predictions = nn.feedforward(test_data)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44951021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
